# Word Embeddings

**_Word embeddings_** is one of the most used techniques in natural language processing (NLP).
It's often said that the performance and ability of State-of-the-art (SOTA) models wouldn't have been possible without
word embeddings.
It's precisely because of word embeddings that language models like RNNs, LSTMs, ELMo, BERT, AlBERT, GPT-2 and GPT-3
have evolved at a staggering pace.

These algorithms are fast and can generate language sequences and other downstream tasks with high accuracy, including
**contextual understanding**, **semantic and syntactic properties**, as well as the **linear relationship** between words.

At the core, these models use embedding as a method to extract patterns from text or voice sequences.
